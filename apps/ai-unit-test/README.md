## 프로젝트 개요

`apps/ai-unit-test`는 사람이 작성한 테스트 코드와 AI가 작성한 테스트 코드를 동일 대상에 대해 비교·분석하기 위한 앱이다. 동일한 컴포넌트/훅/유틸을 대상으로 테스트 설계, 검증 포인트, 커버리지 경향의 차이를 관찰한다.

## 테스트 작성 방식

- AI 테스트는 `.cursor/rules/test.mdc`를 기준 규칙으로 작성했다.
- `.cursor/rules/test.mdc`는 LLM을 이용해 작성되었고, 단위(Unit)·통합(Integration)·E2E까지의 테스트 룰을 “비즈니스 로직 관점”에서 기술하도록 요청해 만들어졌다.

## 파일 명명 규칙

- AI 작성 테스트: 파일명 끝에 `_AI` 접미사 사용. 예) `xxx_AI.spec.tsx`
- 사람 작성 테스트: 일반 파일명. 예) `xxx.spec.tsx`

## 비교 포인트와 특징

- **테스트 포커스**
  - 사람 작성: 실제 사용 시나리오 기반으로 핵심/경계/예외를 압축 검증
  - AI 작성: 규칙을 기반으로 단위→통합→E2E 레벨을 수평적으로 고르게 커버

- **명세 준수도**
  - 사람 작성: 구현 맥락을 알고 있어 전제가 암묵적으로 포함되기도 함
  - AI 작성: 규칙과 화면/도메인 서술을 근거로 가시적 검증 포인트를 구조화

- **케이스 구성**
  - 사람 작성: 중요 경로를 깊게 파고드는 경향
  - AI 작성: 커버리지 지향으로 다양한 변형 케이스를 수평 확장

## 실무 적용성

- `.cursor/rules/test.mdc` 기반 AI 생성 테스트는 “초안 생성 + 누락 포인트 탐색”에 실무 적용 가능하다.
- 사람 작성 테스트와 병행 시, 레벨 간 균형(단위/통합/E2E)과 비즈니스 관점 정합성 점검에 특히 유용하다.
- 최종 품질 보장은 사람 리뷰/리팩터링이 필요하며, 현재 수준은 “초안 자동화 + 품질 갭 점검” 단계다.
